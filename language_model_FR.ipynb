{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e30626a2",
      "metadata": {
        "id": "e30626a2"
      },
      "source": [
        "# NLP Lab : Modèles de langue\n",
        "\n",
        "Dans ce tp, nous allons constuire les briques principales du modèle GPT2 et entrainer un petit modèle sur des poèmes de Victor Hugo.\n",
        "\n",
        "Les questions sont posées dans ce notebook, puis pour executer l'entrainement, il faudra modifier le ficher `gpt_single_head.py` aussi disponible dans le reposository git.\n",
        "\n",
        "\n",
        "## Données\n",
        "\n",
        "Les données d'entrainement sont un recueil de poèmes de Victor Hugo issu du site [gutenberg.org](https://www.gutenberg.org/). Elles sont disponibles dans le répertoire `data`.\n",
        "\n",
        "Afin de réduire la complexité du modèle, nous allons modéliser le texte au niveau caractère. Les modèles de language modélisent généralement des séquences de sous-mots en utilisant des [tokenizers](https://huggingface.co/docs/transformers/tokenizer_summary) (BPE, SentencePiece, WordPiece)\n",
        "\n",
        "Questions :\n",
        ">* En utilisant [collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter), afficher le nombre de caractères différents dans le texte et la fréquence de chaque caractère."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b4d4ab91",
      "metadata": {
        "id": "b4d4ab91",
        "outputId": "052cd6a4-211e-4883-e256-554c1437ae50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of characters in the file: 285222\n",
            "Number of characters in counter: 285222\n",
            "101 different characters\n",
            "Counter({' ': 49127, 'e': 30253, 's': 17987, 'u': 14254, 'r': 14223, 't': 14071, 'a': 14048, 'n': 13725, 'i': 12828, 'o': 12653, 'l': 11638, '\\n': 8102, 'm': 6495, 'd': 6375, ',': 6077, 'c': 5074, 'p': 4206, \"'\": 3820, 'v': 3492, 'é': 2943, 'b': 2783, 'f': 2772, 'h': 2221, 'q': 1956, 'g': 1790, '.': 1420, 'x': 1154, 'L': 1147, '!': 1121, 'E': 1074, ';': 1043, '-': 1020, 'j': 890, 'D': 764, 'è': 725, 'à': 706, 'y': 660, 'I': 627, 'ê': 605, 'C': 593, 'S': 545, 'A': 530, 'Q': 503, 'z': 482, 'J': 471, 'O': 450, 'T': 441, 'P': 435, '?': 388, 'V': 383, 'â': 381, 'N': 362, 'M': 344, 'ù': 298, ':': 294, 'R': 240, 'î': 214, 'U': 208, 'ô': 159, 'X': 150, '1': 146, 'H': 116, 'F': 114, '5': 111, '8': 93, 'B': 78, '«': 74, 'É': 70, '»': 69, 'G': 67, '4': 64, 'û': 62, '3': 47, 'ç': 34, 'À': 33, 'ë': 32, 'ï': 31, '2': 30, '·': 26, 'Ê': 24, '6': 23, '7': 23, 'Ô': 19, '9': 19, 'È': 11, 'k': 10, '0': 10, '_': 8, 'Z': 7, 'Æ': 4, '[': 4, ']': 4, 'w': 3, 'K': 3, 'Y': 3, 'Ë': 2, '(': 2, ')': 2, 'Â': 2, 'Î': 1, 'W': 1})\n",
            "Number of character in counter: 285222\n",
            "101 different characters\n",
            "Counter({' ': 49127, 'e': 30253, 's': 17987, 'u': 14254, 'r': 14223, 't': 14071, 'a': 14048, 'n': 13725, 'i': 12828, 'o': 12653, 'l': 11638, '\\n': 8102, 'm': 6495, 'd': 6375, ',': 6077, 'c': 5074, 'p': 4206, \"'\": 3820, 'v': 3492, 'é': 2943, 'b': 2783, 'f': 2772, 'h': 2221, 'q': 1956, 'g': 1790, '.': 1420, 'x': 1154, 'L': 1147, '!': 1121, 'E': 1074, ';': 1043, '-': 1020, 'j': 890, 'D': 764, 'è': 725, 'à': 706, 'y': 660, 'I': 627, 'ê': 605, 'C': 593, 'S': 545, 'A': 530, 'Q': 503, 'z': 482, 'J': 471, 'O': 450, 'T': 441, 'P': 435, '?': 388, 'V': 383, 'â': 381, 'N': 362, 'M': 344, 'ù': 298, ':': 294, 'R': 240, 'î': 214, 'U': 208, 'ô': 159, 'X': 150, '1': 146, 'H': 116, 'F': 114, '5': 111, '8': 93, 'B': 78, '«': 74, 'É': 70, '»': 69, 'G': 67, '4': 64, 'û': 62, '3': 47, 'ç': 34, 'À': 33, 'ë': 32, 'ï': 31, '2': 30, '·': 26, 'Ê': 24, '6': 23, '7': 23, 'Ô': 19, '9': 19, 'È': 11, 'k': 10, '0': 10, '_': 8, 'Z': 7, 'Æ': 4, '[': 4, ']': 4, 'w': 3, 'K': 3, 'Y': 3, 'Ë': 2, '(': 2, ')': 2, 'Â': 2, 'Î': 1, 'W': 1})\n"
          ]
        }
      ],
      "source": [
        "import collections\n",
        "\n",
        "with open('data/hugo_contemplations.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()   # on nomme les data text ici\n",
        "\n",
        "print(f'Number of characters in the file: {len(text)}')\n",
        "##  YOUR CODE HERE\n",
        "counter = collections.Counter(text)  # Compter la fréquence des caractères\n",
        "\n",
        "chars = counter.keys()  # Récupérer les caractères uniques\n",
        "\n",
        "###\n",
        "\n",
        "print(f'Number of characters in counter: {sum(counter.values())}')\n",
        "print(f'{len(chars)} different characters')\n",
        "print(counter)\n",
        "###\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d80b661f",
      "metadata": {
        "id": "d80b661f"
      },
      "source": [
        "### Encodage / décodage\n",
        "Afin de transformer le texte en vecteur pour le réseau de neurones, il faut encoder chaque caractère avec un entier. Les fonctions suivantes opérent l'encodage et le décodage des caractères :"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Voici le même code mais avec des print juste pour comprendre ce que le modèle fait à chaque étape."
      ],
      "metadata": {
        "id": "-ztf6n9ZiPew"
      },
      "id": "-ztf6n9ZiPew"
    },
    {
      "cell_type": "code",
      "source": [
        "# Créer les dictionnaires d'encodage et de décodage\n",
        "# chars est une liste de tous les caractères uniques trouvés dans le texte.\n",
        "#enumerate(chars) attribue un indice à chaque caractère.\n",
        "#stoi (String-To-Integer) est un dictionnaire qui associe chaque caractère à un nombre unique.\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "## itos (Integer-To-String) est un dictionnaire qui fait l'inverse : il associe chaque nombre à son caractère correspondant.\n",
        "\n",
        "itos = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Afficher une partie des mappings\n",
        "print(\"Exemple de mapping caractères -> entiers (stoi) :\")\n",
        "for k, v in list(stoi.items())[:10]:  # Affiche seulement les 10 premiers pour éviter trop de texte\n",
        "    print(f\"'{k}' -> {v}\")\n",
        "print()\n",
        "\n",
        "print(\"Exemple de mapping entiers -> caractères (itos) :\")\n",
        "for k, v in list(itos.items())[:10]:\n",
        "    print(f\"{k} -> '{v}'\")\n",
        "print()\n",
        "\n",
        "# Définir les fonctions d'encodage et de décodage\n",
        "# La fonction encore transforme une chaîne de caractères s en une liste d'entiers en remplaçant chaque caractère par son index dans stoi.\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "# La fonction decode prend une liste d'entiers et les reconvertit en une chaîne de caractères à l'aide du dictionnaire itos.\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "# Tester avec une chaîne d'exemple\n",
        "testString = \"\\nDemain, dès l'aube\"\n",
        "encoded = encode(testString)\n",
        "decoded = decode(encoded)\n",
        "\n",
        "# Afficher le processus d'encodage et de décodage\n",
        "print(f\"Chaîne de test : {testString}\\n\")\n",
        "print(f\"Encodé : {encoded}\\n\")\n",
        "print(f\"Décodé : {decoded}\\n\")\n",
        "\n",
        "# Vérification de la cohérence\n",
        "assert decoded == testString, \"Erreur: l'encodage/décodage ne fonctionne pas correctement !\"\n",
        "print(\"Le test est réussi : l'encodage et le décodage sont cohérents.\")\n"
      ],
      "metadata": {
        "id": "TLkXRPwjhlpF",
        "outputId": "4d791d96-4033-4112-c24b-80a7dc0e4741",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TLkXRPwjhlpF",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caractères uniques (101): ['L', 'E', 'S', '\\n', 'C', 'O', 'N', 'T', 'M', 'P', 'A', 'I', 'R', 'V', ' ', 'H', 'U', 'G', 'J', 'D', \"'\", '.', '-', '1', '8', '4', '3', '5', '6', 'i', 'n', 'q', 'u', 'è', 'm', 'e', 'É', 'd', 't', 'o', 'Z', 'B', ',', 's', 'r', 'é', 'v', 'Q', 'È', 'Æ', 'c', '!', 'a', 'l', 'x', 'b', 'ù', 'p', 'g', 'â', 'à', 'ê', 'h', 'z', 'f', ';', 'y', 'î', 'j', 'F', 'ô', '?', 'À', 'ï', ':', '«', '»', 'ë', 'û', '2', 'ç', 'X', '7', 'Ê', 'Ë', '_', '(', ')', 'w', '[', ']', 'Ô', 'Â', 'k', 'Î', '·', 'K', '0', 'Y', 'W', '9']\n",
            "\n",
            "Exemple de mapping caractères -> entiers (stoi) :\n",
            "'L' -> 0\n",
            "'E' -> 1\n",
            "'S' -> 2\n",
            "'\n",
            "' -> 3\n",
            "'C' -> 4\n",
            "'O' -> 5\n",
            "'N' -> 6\n",
            "'T' -> 7\n",
            "'M' -> 8\n",
            "'P' -> 9\n",
            "\n",
            "Exemple de mapping entiers -> caractères (itos) :\n",
            "0 -> 'L'\n",
            "1 -> 'E'\n",
            "2 -> 'S'\n",
            "3 -> '\n",
            "'\n",
            "4 -> 'C'\n",
            "5 -> 'O'\n",
            "6 -> 'N'\n",
            "7 -> 'T'\n",
            "8 -> 'M'\n",
            "9 -> 'P'\n",
            "\n",
            "Chaîne de test : \n",
            "Demain, dès l'aube\n",
            "\n",
            "Encodé : [3, 19, 35, 34, 52, 29, 30, 42, 14, 37, 33, 43, 14, 53, 20, 52, 32, 55, 35]\n",
            "\n",
            "Décodé : \n",
            "Demain, dès l'aube\n",
            "\n",
            "Le test est réussi : l'encodage et le décodage sont cohérents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf2a633d",
      "metadata": {
        "id": "bf2a633d"
      },
      "source": [
        "### Découpage Train/Validation\n",
        "\n",
        "L'objectif étant de prédire des poèmes, il ne faut pas mélanger les lignes aléatoirement. Il faut garder l'ordre des lignes dans le texte et uniquement prendre les premiers 90% pour entrainer et les 10% restants pour contrôler l'apprentissage.\n",
        "\n",
        "Questions :\n",
        "> * Découper en `train_data` (90%) et `val_data` (10%) en utilisant du slicing sur data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "bf5b7420",
      "metadata": {
        "id": "bf5b7420",
        "outputId": "f1d442e0-d9fc-4e89-b232-bf6c062ed839",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre total de tokens dans les données encodées : 285222\n",
            "\n",
            "Taille de train_data : 256699 (90.00%)\n",
            "Taille de val_data : 28523 (10.00%)\n",
            "\n",
            "Extrait de train_data (encodé) :\n",
            " [0, 1, 2, 3, 4, 5, 6, 7, 1, 8, 9, 0, 10, 7, 11, 5, 6, 2, 3, 3, 9, 10, 12, 3, 3, 13, 11, 4, 7, 5, 12, 14, 15, 16, 17, 5, 3, 3, 3, 11, 11, 3, 3, 10, 16, 18, 5, 16, 12, 19] \n",
            "\n",
            "Extrait de val_data (encodé) :\n",
            " [35, 38, 14, 44, 35, 64, 53, 45, 38, 52, 30, 38, 14, 53, 35, 43, 14, 50, 29, 35, 32, 54, 65, 3, 1, 38, 42, 14, 53, 52, 14, 30, 32, 29, 38, 42, 14, 37, 52, 30, 43, 14, 53, 20, 45, 38, 52, 38, 14, 34] \n",
            "\n",
            "Extrait de train_data (décodé) :\n",
            " LES\n",
            "CONTEMPLATIONS\n",
            "\n",
            "PAR\n",
            "\n",
            "VICTOR HUGO\n",
            "\n",
            "\n",
            "II\n",
            "\n",
            "AUJOURD \n",
            "\n",
            "Extrait de val_data (décodé) :\n",
            " et reflétant les cieux;\n",
            "Et, la nuit, dans l'état m \n",
            "\n",
            "Derniers caractères de train_data (décodé) :\n",
            " re, et luit, miroir sinistre,\n",
            "Ruisselante de sang  \n",
            "\n",
            "Premiers caractères de val_data (décodé) :\n",
            " et reflétant les cieux;\n",
            "Et, la nuit, dans l'état m \n",
            "\n",
            "Le découpage est correct et conserve l'ordre du texte.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# Train and validation splits\n",
        "# Encoder le texte en une séquence d'entiers\n",
        "data = torch.tensor(encode(text), dtype=torch.long) #Le texte est chargé et transformé en une séquence de nombres entiers (data).\n",
        "\n",
        "\n",
        "# Vérifier la longueur des données encodées\n",
        "print(f\"Nombre total de tokens dans les données encodées : {len(data)}\\n\")\n",
        "## YOUR CODE HERE\n",
        "# Définir la séparation 90% - 10%\n",
        "train_size = int(0.9 * len(data))  # 90% des données pour l'entraînement\n",
        "\n",
        "# Découper les données en train et validation\n",
        "train_data = data[:train_size]  # Les 90% premiers caractères pour l'entraînement\n",
        "val_data = data[train_size:]  # Les 10% restants pour la validation\n",
        "\n",
        "# Vérifier les tailles après découpage\n",
        "print(f\"Taille de train_data : {len(train_data)} ({(len(train_data)/len(data))*100:.2f}%)\")\n",
        "print(f\"Taille de val_data : {len(val_data)} ({(len(val_data)/len(data))*100:.2f}%)\\n\")\n",
        "\n",
        "# Afficher un aperçu des premières valeurs de chaque ensemble\n",
        "print(\"Extrait de train_data (encodé) :\\n\", train_data[:50].tolist(), \"\\n\")\n",
        "print(\"Extrait de val_data (encodé) :\\n\", val_data[:50].tolist(), \"\\n\")\n",
        "\n",
        "# Afficher un aperçu du texte original correspondant après décodage\n",
        "print(\"Extrait de train_data (décodé) :\\n\", decode(train_data[:50].tolist()), \"\\n\")\n",
        "print(\"Extrait de val_data (décodé) :\\n\", decode(val_data[:50].tolist()), \"\\n\")\n",
        "\n",
        "# Vérifier que l'ordre du texte est respecté à la transition train -> validation\n",
        "print(\"Derniers caractères de train_data (décodé) :\\n\", decode(train_data[-50:].tolist()), \"\\n\")\n",
        "print(\"Premiers caractères de val_data (décodé) :\\n\", decode(val_data[:50].tolist()), \"\\n\")\n",
        "\n",
        "# Vérification finale : aucun chevauchement\n",
        "assert decode(train_data[-50:].tolist()) + decode(val_data[:50].tolist()) == decode(data[train_size-50:train_size+50].tolist()), \\\n",
        "    \"Erreur : La séparation des données ne préserve pas la continuité du texte !\"\n",
        "\n",
        "print(\"Le découpage est correct et conserve l'ordre du texte.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text c'est vraiment tous les poèmes à la suite en mots\n",
        "# alors que data c'est la version de text en nombre\n",
        "# data = torch.tensor(encode(text), dtype=torch.long) #Le texte est chargé et transformé en une séquence de nombres entiers (data).\n",
        "# encode(text) transforme chaque caractère du texte en un entier dans le même ordre que dans le texte original.\n",
        "#Cela signifie que data est une version numérique du texte où chaque caractère reste à sa place."
      ],
      "metadata": {
        "id": "uu8rBCWOlFU2"
      },
      "id": "uu8rBCWOlFU2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "id": "1rub1H8ak80l",
        "outputId": "e909ed05-38c0-4508-9700-401433df8f1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "1rub1H8ak80l",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0,  1,  2,  ..., 27,  3,  3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa498280",
      "metadata": {
        "id": "aa498280"
      },
      "source": [
        "### Contexte\n",
        "\n",
        "Le modèle de langue possède comme paramètre la taille maximale du contexte à considérer pour faire la prédiction du prochain caractère. Ce contexte est appelé `block_size`. Les données d'apprentissage sont donc des séquences de caractères consécutifs, issues de l'ensemble d'entraînenement tirées aléatoirement et de longueur `block_size`.\n",
        "\n",
        "block_size =  **nombre de caractères consécutifs que le modèle voit avant de faire une prédiction.**\n",
        "-> si block_size = 1, le modèle ne voit qu'un seul caractère avant de prédire le suivant => trop petit → le modèle ne comprend que des petits motifs locaux.\n",
        "-> Si block_size = 100, le modèle pourrait comprendre des structures plus longues, comme des phrases entières. => si  trop grand → cela augmente la mémoire et le coût du modèle.\n",
        "\n",
        "\n",
        "\n",
        "Si le caractère de début de la séquence est `i`, la séquence de contexte est donc :\n",
        "``` x = data[i:i+block_size]```\n",
        "et la valeur à prédire à chaque position dans le contexte est le caractère suivant :\n",
        "```y = [data[i+1:i+block_size+1]```.\n",
        "\n",
        "\n",
        "Le modèle apprend en recevant progressivement plus d’informations.\n",
        "Il commence avec un seul caractère, puis prédit caractère après caractère.\n",
        "C’est un apprentissage séquentiel, où chaque nouvelle lettre renforce ce que le modèle comprend.\n",
        "L’objectif est d’apprendre à anticiper la suite du texte avec de plus en plus de contexte.\n",
        "\n",
        "On choisit aléatoirement un point de départ dans train_data.\n",
        "✅ On récupère une séquence de block_size caractères (x).\n",
        "✅ On récupère la version décalée d'un caractère en avant (y).\n",
        "✅ On entraîne le modèle à prédire y[t] en lui donnant un contexte croissant x[:t+1]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "97a262bf",
      "metadata": {
        "id": "97a262bf",
        "outputId": "05211ea4-0e14-4f83-9727-ca1e8741ec7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([37612])\n",
            "context is >u< target is >s<\n",
            "context is >us< target is >i<\n",
            "context is >usi< target is >n<\n",
            "context is >usin< target is >s<\n",
            "context is >usins< target is > <\n",
            "context is >usins < target is >q<\n",
            "context is >usins q< target is >u<\n",
            "context is >usins qu< target is >a<\n"
          ]
        }
      ],
      "source": [
        "block_size = 8\n",
        "\n",
        "i  = torch.randint(len(data) - block_size, (1,)) # torch.randint(len(data) - block_size, (1,)) génère un indice aléatoire i, c'est-à-dire le point de départ de la séquence dans train_data.\n",
        "#On s'assure que i est dans les limites du texte en soustrayant block_size à len(data)\n",
        "print (i)\n",
        "x = train_data[i:i+block_size] #x est la séquence de block_size caractères à partir de l’indice i.\n",
        "\n",
        "y = train_data[i+1:i+1+block_size] # y est la même séquence, décalée d’un caractère vers la droite.\n",
        "#À chaque position, y contient le prochain caractère attendu.\n",
        "\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print (f'context is >{decode(context.tolist())}< target is >{decode([target.tolist()])}<')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0de81464",
      "metadata": {
        "id": "0de81464"
      },
      "source": [
        "### Définition des batchs\n",
        "\n",
        "Les batchs d'entrainement sont constitués de plusieurs séquences de caractères tirées aléatoirement dans `train_data`. Pour choisir aléatoirement une séquence à mettre dans le batch, il faut tirer aléatoirement un point de départ dans `train_data` et extraire les `block_size` caractères suivants. Lors du tirage du point de départ, faire attention à laisser suffisamment de caractères après le point de départ pour avoir une séquence de `block_size` caractères.\n",
        "\n",
        "Questions :\n",
        "> * Créer les batchs `x` en tirant `batch_size` séquences de longeur `block_size` à  partir d'un point `i` tiré aléatoirement. Empiler les exemples avec `torch.stack`.\n",
        "> * Créer les batchs `y` en ajoutant le caractère suivant la séquence `x`. Empiler les exemples avec `torch.stack`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9be91965",
      "metadata": {
        "id": "9be91965"
      },
      "outputs": [],
      "source": [
        "batch_size = 4\n",
        "torch.manual_seed(2023)\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ## YOUR CODE HERE\n",
        "    # select batch_size starting points in the data, store them in a list called starting_points\n",
        "\n",
        "    # x is the sequence of integer starting at each straing point and of length block_size\n",
        "\n",
        "    # y is the character after each starting position\n",
        "\n",
        "    ###\n",
        "    # send data and target to device\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6507313b",
      "metadata": {
        "id": "6507313b"
      },
      "source": [
        "### Premier modèle : un bigramme\n",
        "\n",
        "Le premier modèle que nous allons implémenter est un modèle bigramme. Il prédit le caractère suivant uniquement en fonction du caractère courant. Il est possible de stocker ce modèle dans une simple matrice : pour chaque caractère (en ligne), on stocke la distribution de probabilités sur l'ensemble des caractères suivants (en colonne). On peut donc le stocker dans une simple couche [`Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html).\n",
        "\n",
        "Questions :\n",
        "> * Dans le constructeur, définir une couche Embedding de taille `vocab_size` par `vocab_size`.\n",
        "> * Dans forward, appliquer la couche d'embedding au batch de idx (`x`).\n",
        "> * Dans forward, définir la loss comme la `cross_entropy` entre la prédiction et target (`y`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b7a7478",
      "metadata": {
        "id": "9b7a7478"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# use a gpu if we have one\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# Simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # we use a simple vocab_size times vocab_size tensor to store the probabilities\n",
        "        # of each token given a single token as context in nn.Embedding\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        ##\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "\n",
        "        # idx and targets are both (Batch,Time) tensor of integers\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        ##\n",
        "\n",
        "        # don't compute loss if we don't have targets\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # change the shape of the logits and target to match what is needed for CrossEntropyLoss\n",
        "            # https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
        "            Batch, Time, Channels = logits.shape\n",
        "            logits = logits.view(Batch*Time, Channels)\n",
        "            targets = targets.view(Batch*Time)\n",
        "\n",
        "            # negative log likelihood between prediction and target\n",
        "            # YOUR CODE HERE\n",
        "\n",
        "            ##\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = nn.functional.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "# send the model to device\n",
        "m = model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0e3cffb",
      "metadata": {
        "id": "f0e3cffb"
      },
      "source": [
        "#### Modèle avant entraînement\n",
        "Le modèle n'a pas encore été entrainé, il est juste initialisé, mais on peut calculer la loss sur un batch aléatoire. Les poids étant initialisés avec une distribution normale N(0,1) sur chaque dimension, la loss attendue après l'initialisation devrait être proche de `-ln(1/vocab_size)` (l'entropie est maximale)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "62343cbc",
      "metadata": {
        "id": "62343cbc"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "xb, yb = get_batch('train')\n",
        "logits, loss = m(xb, yb)\n",
        "print (logits.shape)\n",
        "print (f'loss attendue {-math.log(1.0/vocab_size)}')\n",
        "print (f'loss calculée {loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0d44b50",
      "metadata": {
        "id": "d0d44b50"
      },
      "source": [
        "Pour utiliser le modèle en prédiction, il faut lui fournir un premier caractère pour amorcer la séquence : c'est le prompt. Dans notre cas, on peut initialiser la génération avec le caractère de retour à la ligne pour débuter une nouvelle phrase.\n",
        "\n",
        "Questions :\n",
        "> * Créer un prompt avec un tenseur de taille (1,1) contenant l'entier correspondant au caractère `\\n`.\n",
        "> * Générer une séquence de caractères de taille 100 à partir de ce prompt avec les fonctions `m.generate` et `decode`.\n",
        "> * Comment est la phrase générée ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "320d3ead",
      "metadata": {
        "id": "320d3ead"
      },
      "outputs": [],
      "source": [
        "print (encode(['\\n']))\n",
        "## YOUR CODE HERE\n",
        "\n",
        "\n",
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3630d8c3",
      "metadata": {
        "id": "3630d8c3"
      },
      "source": [
        "### Entrainement\n",
        "\n",
        "Pour l'entrainement, nous utilisons un optimiseur [AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) avec un learning rate de 1e-3. Une itération d'apprentissage consiste en\n",
        "- générer un batch\n",
        "- appliquer le réseau de neurones (forward) et calculer la loss (`model(xb, yb)`)\n",
        "- calculer le gradient (après avoir remis à zero le gradient cumulé) (`loss.backward()`)\n",
        "- mettre à jour les paramètres (`optimizer.step()`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05831b58",
      "metadata": {
        "id": "05831b58"
      },
      "outputs": [],
      "source": [
        "max_iters = 100\n",
        "batch_size = 4\n",
        "eval_interval = 10\n",
        "learning_rate = 1e-3\n",
        "eval_iters = 20\n",
        "\n",
        "@torch.no_grad() # no gradient is computed here\n",
        "def estimate_loss():\n",
        "    \"\"\" Estimate the loss on eval_iters batch of train and val sets.\"\"\"\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# re-create the model\n",
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "512395a0",
      "metadata": {
        "id": "512395a0"
      },
      "source": [
        "Une fois le réseau entrainé pendant 100 itérations, on peut générer une séquence de caractères.\n",
        "\n",
        "Questions :\n",
        "> * Quel est l'effet de l'entraînement ?\n",
        "> * Augmenter le nombre d'itérations à 1000 puis à 10,000, noter la loss obtenue et la phrase générée. Qu'observez-vous ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4071b998",
      "metadata": {
        "id": "4071b998"
      },
      "outputs": [],
      "source": [
        "idx = torch.ones((1,1), dtype=torch.long)*3\n",
        "print (decode(m.generate(idx, max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dfd33b1",
      "metadata": {
        "id": "5dfd33b1"
      },
      "source": [
        "## Single Head Attention\n",
        "\n",
        "Nous allons maintenant implémenter le mécanisme de base de l'attention. Pour chaque couple de mots de la séquence, ce mécanisme combine Q une *query* (l'information recherchée), K une *key* (l'information obtenue) et calcule  V une *value*, un vecteur de résultat.\n",
        "\n",
        "![single head attention](https://github.com/MaelaGLG/nlp-lab-language-models/blob/main/images/single_head_attention.png?raw=1)\n",
        "\n",
        "### Masquage\n",
        "Cependant, comme nous utilisons le modèle pour générer des séquences, on ne doit pas utiliser les caractères situés après le caractère courant, car ce sont justement ces caractères que l'on cherche à prédire lors de l'apprentissage : *le futur n'est pas utilisé pour prédire (le futur).*\n",
        "\n",
        "On va donc intégrer une matrice de masquage dans le processus. Cette matrice indique que pour le premier caractère de la séquence, on ne peux utiliser que ce caractère pour prédire (pas de contexte). Pour le second caractère, on peut utiliser le premier caractère et le second. Pour le troisième caractère, on peut utiliser le premier, le second et le troisième et ainsi de suite. Cette matrice est donc une matrice triangulaire inférieure. Cette matrice est normalisée par ligne (les lignes somment à 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d15fbb1d",
      "metadata": {
        "id": "d15fbb1d"
      },
      "outputs": [],
      "source": [
        "T = 8\n",
        "\n",
        "# first version of the contraints with matrix multiplication\n",
        "# create a lower triangular matrix\n",
        "weights0 = torch.tril(torch.ones(T,T))\n",
        "# normalize each row\n",
        "weights0 = weights0 / weights0.sum(1, keepdim=True)\n",
        "print (weights0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f1eb4a7",
      "metadata": {
        "id": "1f1eb4a7"
      },
      "source": [
        "La couche [`softmax`](https://pytorch.org/docs/stable/generated/torch.nn.functional.softmax.html) est une autre manière de réaliser la normalisation :\n",
        "\n",
        "Question :\n",
        "> * Vérifier qu'on obtient bien la même matrice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75455f0e",
      "metadata": {
        "id": "75455f0e"
      },
      "outputs": [],
      "source": [
        "tril = torch.tril(torch.ones(T,T))\n",
        "weights = torch.tril(torch.ones(T,T))\n",
        "weights = weights.masked_fill(tril== 0, float('-inf'))\n",
        "weights = nn.functional.softmax(weights, dim=-1)\n",
        "print (weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f76c8b",
      "metadata": {
        "id": "30f76c8b"
      },
      "source": [
        "### Implémentation\n",
        "\n",
        "Nous pouvons maintenant implémenter la couche d'attention :\n",
        "\n",
        "![attention_formula](https://github.com/MaelaGLG/nlp-lab-language-models/blob/main/images/attention_formula.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02681533",
      "metadata": {
        "id": "02681533"
      },
      "source": [
        "Questions :\n",
        "\n",
        "> * Créer les couches key, query et value comme des couches linéaires de dimension `C` x `head_size`.\n",
        "> * Appliquer les couches à `x`.\n",
        "> * `weights = query x key` (transposer les deuxième et troisième dimensions de key pour pouvoir faire le produit).\n",
        "> * Appliquer le facteur de normalisation.\n",
        "> * Appliquer le masque triangulaire et la softmax à `weights`.\n",
        "> * Appliquer value à `x`.\n",
        "> * Le résultat `out` est la multiplication de `weights` par `value(x)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "129fe994",
      "metadata": {
        "id": "129fe994"
      },
      "outputs": [],
      "source": [
        "head_size = 16\n",
        "B, T, C = 4, 8, 32\n",
        "x = torch.randn(B, T, C)\n",
        "## YOUR CODE HERE\n",
        "# define the Key layer\n",
        "key =\n",
        "# define the Query layer\n",
        "query =\n",
        "# define the Value layer\n",
        "value =\n",
        "# apply each layer to the input\n",
        "k =  # (B, T, head_size)\n",
        "q =  # (B, T, head_size)\n",
        "v =  # (B, T, head_size)\n",
        "# compute the normalize product between Q and K\n",
        "weights =  # (B, T, head_size) @ (B, 16, head_size) -> (B, T, T)\n",
        "# apply the mask (lower triangular matrix)\n",
        "weights = weights.masked_fill(tril== 0, float('-inf'))\n",
        "# apply the softmax\n",
        "weights =\n",
        "###\n",
        "out  = weights @ value(x) # (B, T, head_size)\n",
        "\n",
        "# print the result\n",
        "weights[0]\n",
        "out[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db591771",
      "metadata": {
        "id": "db591771"
      },
      "source": [
        "Questions :\n",
        "\n",
        "> * Copier votre code dans `gpt_single_head.py` : la définition des couches dans le constructeur de la classe `Head` et les calculs dans la fonction `forward`.\n",
        "> * Faire un entrainement.\n",
        "> * Quelle loss en train et val obtenez vous ? Le texte vous parait-il meilleur ?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c043812e",
      "metadata": {
        "id": "c043812e"
      },
      "source": [
        "## Multi-head attention\n",
        "\n",
        "La *multi-head attention* est simplement le calcul en parallèle de plusieurs *single head attention*. Chacune des single head attention est concaténée pour créer la sortie de la multi-head attention. Dans la figure issue de l'article original, le nombre de *heads* dans le *multi-head* est `h`. Afin d'opérer des combinaisons pondérées sur la sortie de chacune des single head, une couche de calcul linéaire est ajoutée.\n",
        "\n",
        "![multi head attention](https://github.com/MaelaGLG/nlp-lab-language-models/blob/main/images/multi_head_attention.png?raw=1)\n",
        "\n",
        "Le code ci-dessous crée un module de multi-head attention.\n",
        "Questions :\n",
        "> * Dans le constructeur, créer une liste contenant `num_heads` module `Head` en utilisant la fonction [ModuleList](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html) de pytorch.\n",
        "> * Dans la fonction `forward`, appliquer chaque single head à l'input et concaténer le résultat en utilisant la fonction [cat](https://pytorch.org/docs/stable/generated/torch.cat.html) de pytorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fab8977",
      "metadata": {
        "id": "5fab8977"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        ## YOUR CODE HERE\n",
        "        ## list of num_heads modules of type Head\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        ###\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## YOUR CODE HERE\n",
        "        ## apply each head in self.heads to x and concat the results\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56a39989",
      "metadata": {
        "id": "56a39989"
      },
      "source": [
        "Questions :\n",
        "> * Copier le fichier `gpt_single_head.py` en `gpt_multi_head.py`.\n",
        "> * Ajouter le module MultiHeadAttention dans `gpt_multi_head.py`.\n",
        "> * En tête de fichier, ajouter un paramètre  `n_head = 4`.\n",
        "> * Dans le module BigramLanguageModel, remplacer le module Head par un module MultiHeadAttention avec les paramètres `num_heads = n_head` et `head_size = n_embd // n_head` pour garder le même nombre de paramètres.\n",
        "> Relancer l'entrainement et noter le nombre de paramètres et les loss obtenues.\n",
        "\n",
        "0.009893 M parameters\n",
        "step 4999: train loss 2.1570, val loss 2.1802"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "333d3f27",
      "metadata": {
        "id": "333d3f27"
      },
      "source": [
        "## Ajout d'une couche de calcul FeedForward\n",
        "\n",
        "\n",
        "Après les couches d'attention qui collectent l'information dans la séquence, une couche de calcul est ajoutée pour combiner toutes les informations de la séquence. Cette couche est un simple Multi-Layer-Perceptron avec une couche cachée et une non linéarité de type [RELU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html).\n",
        "![multi feedfoward](https://github.com/MaelaGLG/nlp-lab-language-models/blob/main/images/multi_ff.png?raw=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "915f8148",
      "metadata": {
        "id": "915f8148"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"\"\" a simple MLP with RELU \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, n_embd),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0ca5ef7",
      "metadata": {
        "id": "f0ca5ef7"
      },
      "source": [
        "Questions :\n",
        "> * Ajouter le module `FeedForward` dans votre fichier `gpt_multi_head.py`.\n",
        "> * Ajouter cette couche `FeedForward` après la *multi-head attention*.\n",
        "> * Relancer l'entrainement et noter le nombre de paramètres et les loss obtenues."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5942f7be",
      "metadata": {
        "id": "5942f7be"
      },
      "source": [
        "0.010949 M parameters\n",
        "step 4999: train loss 2.1290, val loss 2.1216"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc16dfb3",
      "metadata": {
        "id": "bc16dfb3"
      },
      "source": [
        "## Empiler les blocs\n",
        "\n",
        "Le réseau construit jusqu'à présent n'est en fait qu'un bloc du réseau final. Il est maintenant possible d'empiler les blocs de *multi-head attention* pour créer un réseau profond.\n",
        "\n",
        "![multi feedfoward](https://github.com/MaelaGLG/nlp-lab-language-models/blob/main/images/multi_bloc.png?raw=1)\n",
        "\n",
        "\n",
        "Le code suivant crée un bloc :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fbdecf5",
      "metadata": {
        "id": "5fbdecf5"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" A single bloc of multi-head attention \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.sa(x)\n",
        "        x = self.ffwd(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3144fff7",
      "metadata": {
        "id": "3144fff7"
      },
      "source": [
        "Questions :\n",
        "> * Ajouter le module `Block` dans `gpt_multi_head.py`.\n",
        "> * Modifier le code de `BigramLanguageModel` pour ajouter 3 `Block(n_embd, n_head=4)` avec un container [Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html) à la place de `MultiHeadAttention`et `FeedForward`.\n",
        "> * Relancer l'entrainement et noter le nombre de paramètres et les loss obtenues.\n",
        "\n",
        "0.019205 M parameters\n",
        "step 4999: train loss 2.2080, val loss 2.2213"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be02d77a",
      "metadata": {
        "id": "be02d77a"
      },
      "source": [
        "## Amélioration de l'entraînement\n",
        "\n",
        "Si on veut continuer à augmenter la taille du réseau, il est nécessaire d'utiliser des couches permettant d'améliorer l'entraînement et ses capacités de généralisation (réduire le sur-apprentissage). Ces couches sont :\n",
        "- *skip connections* ou *residual connections*\n",
        "- les couches de normalisation\n",
        "- le dropout.\n",
        "\n",
        "\n",
        "![multi feedfoward](https://github.com/MaelaGLG/nlp-lab-language-models/blob/main/images/multi_skip_norm.png?raw=1)\n",
        "\n",
        "\n",
        "Questions :\n",
        "> * Dans le module Block, ajouter une skip connection en ajoutant l'input dans chaque connexion :\n",
        "```\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "```\n",
        "> * Dans le module Block, ajouter 2 couches de [LayerNorm](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html) de taille `n_embd` avant la couche de `Multi-Head attention` et avant la `FeedForward`.\n",
        "> * Après la série de 3 blocs, ajouter une couche de LayerNorm de taille `n_embd`.\n",
        "> * Définir une variable `dropout = 0.2` en début de fichier et ajouter une couche de [Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) :\n",
        ">    * Après la couche RELU dans FeedForward\n",
        ">    * Après la couche de MultiHead dans `MultiHeadAttention`\n",
        ">    * Après la softmax dans la single head attention `Head`.\n",
        "> * Relancer l'entrainement et noter le nombre de paramètres et les loss obtenues.\n",
        "\n",
        "0.019653 M parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4733a17",
      "metadata": {
        "id": "d4733a17"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Les principaux éléments de GPT2 sont en place, il faut maintenant faire passer le modèle à l'échelle et l'entraîner sur une base de données beaucoup plus grande. Pour comparaison, les paramètres de [GPT2](https://huggingface.co/transformers/v2.11.0/model_doc/gpt2.html) sont :\n",
        "\n",
        "* `vocab_size = 50257` : GPT2 modélise des tokens (subwords) alors que nous modélisons des caractères. Pour nous, `vocab_size = 100`.\n",
        "* `n_positions = 1024` : la taille maximale du contexte. Pour nous, c'est `block_size = 8`.\n",
        "* `n_embd = 768`:  la dimension des embeddings. Pour nous c'est `n_embd = 32`.\n",
        "* `n_layer = 12`: le nombre de block. Pour nous c'est 3.\n",
        "* `n_head = 12`: le nombre de multi-head attention. Pour nous c'est 4.\n",
        "\n",
        "Au total, GPT2 est composé de 1,500 millions de paramètres et a été entrainé sur 8M de pages web, soit 40 Gb de texte.\n",
        "\n",
        "\n",
        "```\n",
        "10.816613 M parameters\n",
        "step 0: train loss 4.7847, val loss 4.7701\n",
        "step 4999: train loss 0.2683, val loss 2.1161\n",
        "time: 31m47.910s   \n",
        "    \n",
        "Le pêcheur où l'homme en peu de Carevante\n",
        "Sa conter des chosses qu'en ses yoitn!\n",
        "\n",
        "Ils sont là-hauts parler à leurs ténèbres\n",
        "A ceux qu'on rêve aux oiseaux des cheveux,\n",
        "Et celus qu'on tourna jamais sous le front;\n",
        "Ils se disent tu mêle aux univers.\n",
        "J'ai vu Jean vu France, potte; petits contempler,\n",
        "Et petié calme au milibre et versait,\n",
        "M'éblouissant, emportant, écoute, ingorancessible,\n",
        "On meurt s'efferayait.....--Pas cont âme parle en Apparia!\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "lm-env",
      "language": "python",
      "name": "lm-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}